{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "Supervised and Self-Supervised.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyPYfCGly7cD"
      },
      "source": [
        "Supervised and Self-Supervised tasks\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQXUGeelotmm"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P06D5QgaoyGx"
      },
      "source": [
        "# Dataset hyperparameters\n",
        "unlabeled_dataset_size = 100000\n",
        "labeled_dataset_size = 5000\n",
        "image_size = 96\n",
        "image_channels = 3\n",
        "\n",
        "# Algorithm hyperparameters\n",
        "num_epochs = 20\n",
        "batch_size = 525  # Corresponds to 200 steps per epoch\n",
        "width = 128\n",
        "temperature = 0.1\n",
        "# Stronger augmentations for contrastive, weaker ones for supervised training\n",
        "contrastive_augmentation = {\"min_area\": 0.25, \"brightness\": 0.6, \"jitter\": 0.2}\n",
        "classification_augmentation = {\"min_area\": 0.75, \"brightness\": 0.3, \"jitter\": 0.1}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHADiV7dlnbK"
      },
      "source": [
        "def prepare_dataset():\n",
        "    # Labeled and unlabeled samples are loaded synchronously\n",
        "    # with batch sizes selected accordingly\n",
        "    steps_per_epoch = (unlabeled_dataset_size + labeled_dataset_size) // batch_size\n",
        "    unlabeled_batch_size = unlabeled_dataset_size // steps_per_epoch\n",
        "    labeled_batch_size = labeled_dataset_size // steps_per_epoch\n",
        "    print(\n",
        "        f\"batch size is {unlabeled_batch_size} (unlabeled) + {labeled_batch_size} (labeled)\"\n",
        "    )\n",
        "\n",
        "    unlabeled_train_dataset = (\n",
        "        tfds.load(\"stl10\", split=\"unlabelled\", as_supervised=True, shuffle_files=True)\n",
        "        .shuffle(buffer_size=10 * unlabeled_batch_size)\n",
        "        .batch(unlabeled_batch_size)\n",
        "    )\n",
        "    labeled_train_dataset = (\n",
        "        tfds.load(\"stl10\", split=\"train\", as_supervised=True, shuffle_files=True)\n",
        "        .shuffle(buffer_size=10 * labeled_batch_size)\n",
        "        .batch(labeled_batch_size)\n",
        "    )\n",
        "    test_dataset = (\n",
        "        tfds.load(\"stl10\", split=\"test\", as_supervised=True)\n",
        "        .batch(batch_size)\n",
        "        .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "    )\n",
        "\n",
        "    # Labeled and unlabeled datasets are zipped together\n",
        "    train_dataset = tf.data.Dataset.zip(\n",
        "        (unlabeled_train_dataset, labeled_train_dataset)\n",
        "    ).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "    return train_dataset, labeled_train_dataset, test_dataset\n",
        "\n",
        "\n",
        "# Load STL10 dataset\n",
        "train_dataset, labeled_train_dataset, test_dataset = prepare_dataset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctLAEZebtSXf"
      },
      "source": [
        "# Distorts the color distibutions of images\n",
        "class RandomColorAffine(layers.Layer):\n",
        "    def __init__(self, brightness=0, jitter=0, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.brightness = brightness\n",
        "        self.jitter = jitter\n",
        "\n",
        "    def call(self, images, training=True):\n",
        "        if training:\n",
        "            batch_size = tf.shape(images)[0]\n",
        "\n",
        "            # Same for all colors\n",
        "            brightness_scales = 1 + tf.random.uniform(\n",
        "                (batch_size, 1, 1, 1), minval=-self.brightness, maxval=self.brightness\n",
        "            )\n",
        "            # Different for all colors\n",
        "            jitter_matrices = tf.random.uniform(\n",
        "                (batch_size, 1, 3, 3), minval=-self.jitter, maxval=self.jitter\n",
        "            )\n",
        "\n",
        "            color_transforms = (\n",
        "                tf.eye(3, batch_shape=[batch_size, 1]) * brightness_scales\n",
        "                + jitter_matrices\n",
        "            )\n",
        "            images = tf.clip_by_value(tf.matmul(images, color_transforms), 0, 1)\n",
        "        return images\n",
        "\n",
        "\n",
        "# Image augmentation module\n",
        "def get_augmenter(min_area, brightness, jitter):\n",
        "    zoom_factor = 1.0 - tf.sqrt(min_area)\n",
        "    return keras.Sequential(\n",
        "        [\n",
        "            keras.Input(shape=(image_size, image_size, image_channels)),\n",
        "            preprocessing.Rescaling(1 / 255),\n",
        "            preprocessing.RandomFlip(\"horizontal\"),\n",
        "            preprocessing.RandomTranslation(zoom_factor / 2, zoom_factor / 2),\n",
        "            preprocessing.RandomZoom((-zoom_factor, 0.0), (-zoom_factor, 0.0)),\n",
        "            RandomColorAffine(brightness, jitter),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "\n",
        "def visualize_augmentations(num_images):\n",
        "    # Sample a batch from a dataset\n",
        "    images = next(iter(train_dataset))[0][0][:num_images]\n",
        "    # Apply augmentations\n",
        "    augmented_images = zip(\n",
        "        images,\n",
        "        get_augmenter(**classification_augmentation)(images),\n",
        "        get_augmenter(**contrastive_augmentation)(images),\n",
        "        get_augmenter(**contrastive_augmentation)(images),\n",
        "    )\n",
        "    row_titles = [\n",
        "        \"Original:\",\n",
        "        \"Weakly augmented:\",\n",
        "        \"Strongly augmented:\",\n",
        "        \"Strongly augmented:\",\n",
        "    ]\n",
        "    plt.figure(figsize=(num_images * 2.2, 4 * 2.2), dpi=100)\n",
        "    for column, image_row in enumerate(augmented_images):\n",
        "        for row, image in enumerate(image_row):\n",
        "            plt.subplot(4, num_images, row * num_images + column + 1)\n",
        "            plt.imshow(image)\n",
        "            if column == 0:\n",
        "                plt.title(row_titles[row], loc=\"left\")\n",
        "            plt.axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "\n",
        "\n",
        "visualize_augmentations(num_images=8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GHHTYhSm7d-"
      },
      "source": [
        "Supervised"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCZu4J06Oxzf",
        "outputId": "da47548a-8b2d-42d2-a4cd-b3452441209b"
      },
      "source": [
        "def prepare_dataset():\n",
        "    # Labeled and unlabeled samples are loaded synchronously\n",
        "    # with batch sizes selected accordingly\n",
        "    steps_per_epoch = (unlabeled_dataset_size + labeled_dataset_size) // batch_size\n",
        "    unlabeled_batch_size = unlabeled_dataset_size // steps_per_epoch\n",
        "    labeled_batch_size = labeled_dataset_size // steps_per_epoch\n",
        "    print(\n",
        "        f\"batch size is {unlabeled_batch_size} (unlabeled) + {labeled_batch_size} (labeled)\"\n",
        "    )\n",
        "\n",
        "    unlabeled_train_dataset = (\n",
        "        tfds.load(\"stl10\", split=\"unlabelled\", as_supervised=True, shuffle_files=True)\n",
        "        .shuffle(buffer_size=10 * unlabeled_batch_size)\n",
        "        .batch(unlabeled_batch_size)\n",
        "    )\n",
        "    labeled_train_dataset = (\n",
        "        tfds.load(\"stl10\", split=\"train\", as_supervised=True, shuffle_files=True)\n",
        "        .shuffle(buffer_size=10 * labeled_batch_size)\n",
        "        .batch(labeled_batch_size)\n",
        "    )\n",
        "    test_dataset = (\n",
        "        tfds.load(\"stl10\", split=\"test\", as_supervised=True)\n",
        "        .batch(batch_size)\n",
        "        .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "    )\n",
        "\n",
        "    # Labeled and unlabeled datasets are zipped together\n",
        "    # train_dataset = tf.data.Dataset.zip(\n",
        "    #     (unlabeled_train_dataset, labeled_train_dataset)\n",
        "    # ).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "    return unlabeled_train_dataset, labeled_train_dataset, test_dataset\n",
        "\n",
        "\n",
        "# Load STL10 dataset\n",
        "unlabeled_train_dataset, labeled_train_dataset, test_dataset = prepare_dataset()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "batch size is 500 (unlabeled) + 25 (labeled)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GBPnTaktfjE"
      },
      "source": [
        "# Define the encoder architecture\n",
        "def get_encoder():\n",
        "    return keras.Sequential(\n",
        "        [\n",
        "            keras.Input(shape=(image_size, image_size, image_channels)),\n",
        "            layers.Conv2D(width, kernel_size=3, strides=2, activation=\"relu\"),\n",
        "            layers.Conv2D(width, kernel_size=3, strides=2, activation=\"relu\"),\n",
        "            layers.Conv2D(width, kernel_size=3, strides=2, activation=\"relu\"),\n",
        "            layers.Conv2D(width, kernel_size=3, strides=2, activation=\"relu\"),\n",
        "            # tf.keras.applications.ResNet50(include_top=False, input_shape=(96,96,3)),\n",
        "\n",
        "            layers.Flatten(),\n",
        "            # layers.Dense(width, activation=\"relu\"),\n",
        "        ],\n",
        "        name=\"encoder\",\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWb05cZzHnCt",
        "outputId": "e43906ac-55a6-45c9-e24b-3ed61231c390"
      },
      "source": [
        "baseline_model = keras.Sequential(\n",
        "    [\n",
        "        # keras.Input(shape=(image_size, image_size, image_channels)),\n",
        "        # get_augmenter(**classification_augmentation),\n",
        "        get_encoder(),\n",
        "\n",
        "        layers.Dense(10, activation='softmax'),\n",
        "    ],\n",
        "    name=\"baseline_model\",\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJEO5v0OMaD4",
        "outputId": "ab2ebabb-e013-4fbd-cd9d-c7a2ef14dbed"
      },
      "source": [
        "model = keras.Sequential()\n",
        "model.add(keras.Input(shape=(image_size, image_size, image_channels)))\n",
        "# model.add(layers.Conv2D(width, kernel_size=3, strides=2, activation=\"relu\"))\n",
        "# model.add(layers.Conv2D(width, kernel_size=3, strides=2, activation=\"relu\"))\n",
        "# model.add(layers.Conv2D(width, kernel_size=3, strides=2, activation=\"relu\"))\n",
        "# model.add(layers.Conv2D(width, kernel_size=3, strides=2, activation=\"relu\"))\n",
        "model.add(tf.keras.applications.ResNet50(include_top=False, input_shape=(96,96,3)))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dfa1kKZuHoBN"
      },
      "source": [
        "import tensorflow as tf\n",
        "from keras.utils.vis_utils import plot_model\n",
        "# from tf.keras.utils.plot_model import plot_model\n",
        "# from tensorflow.keras.utils import plot_model\n",
        "# plot_model(model, to_file='model_plot.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SMlW7I9urav",
        "outputId": "45d5c4b9-3090-4e37-c71c-38a538a731dc"
      },
      "source": [
        "labeled_train_dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((None, 96, 96, 3), (None,)), types: (tf.uint8, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyoBSpDbzouM"
      },
      "source": [
        "Supervised"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3oDtItz_orTf",
        "outputId": "ec914c52-2b1f-470d-9136-2561d29aa7d5"
      },
      "source": [
        "# Baseline supervised training with random initialization\n",
        "baseline_model = keras.Sequential(\n",
        "    [\n",
        "        # keras.Input(shape=(image_size, image_size, image_channels)),\n",
        "        # get_augmenter(**classification_augmentation),\n",
        "        keras.Input(shape=(96, 96, 3)),\n",
        "        layers.Conv2D(width, kernel_size=3, strides=2, activation=\"relu\"),\n",
        "        layers.Conv2D(width, kernel_size=3, strides=2, activation=\"relu\"),\n",
        "        layers.Conv2D(width, kernel_size=3, strides=2, activation=\"relu\"),\n",
        "        layers.Conv2D(width, kernel_size=3, strides=2, activation=\"relu\"),\n",
        "            # tf.keras.applications.ResNet50(include_top=False, input_shape=(96,96,3)),\n",
        "\n",
        "        layers.Flatten(),\n",
        "\n",
        "        layers.Dense(10),\n",
        "    ],\n",
        "    name=\"baseline_model\",\n",
        ")\n",
        "baseline_model.compile(\n",
        "    optimizer=keras.optimizers.Adam(),\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")],\n",
        ")\n",
        "\n",
        "baseline_history = baseline_model.fit(\n",
        "    labeled_train_dataset, epochs=num_epochs, validation_data=test_dataset\n",
        ")\n",
        "print(\n",
        "    \"Maximal validation accuracy: {:.2f}%\".format(\n",
        "        max(baseline_history.history[\"val_acc\"]) * 100\n",
        "    )\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "200/200 [==============================] - 117s 583ms/step - loss: 2.4059 - acc: 0.2732 - val_loss: 1.6514 - val_acc: 0.3809\n",
            "Epoch 2/20\n",
            "200/200 [==============================] - 117s 584ms/step - loss: 1.6042 - acc: 0.4116 - val_loss: 1.5232 - val_acc: 0.4389\n",
            "Epoch 3/20\n",
            "200/200 [==============================] - 116s 582ms/step - loss: 1.4189 - acc: 0.4764 - val_loss: 1.6424 - val_acc: 0.4198\n",
            "Epoch 4/20\n",
            "200/200 [==============================] - 117s 583ms/step - loss: 1.2389 - acc: 0.5420 - val_loss: 1.6457 - val_acc: 0.4455\n",
            "Epoch 5/20\n",
            "200/200 [==============================] - 117s 584ms/step - loss: 1.0624 - acc: 0.6134 - val_loss: 1.7973 - val_acc: 0.4428\n",
            "Epoch 6/20\n",
            "200/200 [==============================] - 118s 591ms/step - loss: 0.8600 - acc: 0.6932 - val_loss: 2.0834 - val_acc: 0.4428\n",
            "Epoch 7/20\n",
            "200/200 [==============================] - 117s 585ms/step - loss: 0.7162 - acc: 0.7514 - val_loss: 2.3355 - val_acc: 0.4430\n",
            "Epoch 8/20\n",
            "200/200 [==============================] - 116s 580ms/step - loss: 0.5769 - acc: 0.7960 - val_loss: 2.4674 - val_acc: 0.4296\n",
            "Epoch 9/20\n",
            "200/200 [==============================] - 116s 581ms/step - loss: 0.5068 - acc: 0.8274 - val_loss: 2.5035 - val_acc: 0.4279\n",
            "Epoch 10/20\n",
            "200/200 [==============================] - 118s 588ms/step - loss: 0.4704 - acc: 0.8446 - val_loss: 3.0969 - val_acc: 0.4181\n",
            "Epoch 11/20\n",
            "200/200 [==============================] - 117s 587ms/step - loss: 0.3859 - acc: 0.8698 - val_loss: 3.1921 - val_acc: 0.4225\n",
            "Epoch 12/20\n",
            "200/200 [==============================] - 117s 586ms/step - loss: 0.2984 - acc: 0.9002 - val_loss: 3.9917 - val_acc: 0.4174\n",
            "Epoch 13/20\n",
            "200/200 [==============================] - 117s 585ms/step - loss: 0.2951 - acc: 0.9072 - val_loss: 3.6573 - val_acc: 0.4269\n",
            "Epoch 14/20\n",
            "200/200 [==============================] - 116s 582ms/step - loss: 0.2793 - acc: 0.9156 - val_loss: 4.6129 - val_acc: 0.4214\n",
            "Epoch 15/20\n",
            "200/200 [==============================] - 118s 588ms/step - loss: 0.2716 - acc: 0.9142 - val_loss: 4.1229 - val_acc: 0.4279\n",
            "Epoch 16/20\n",
            "200/200 [==============================] - 117s 584ms/step - loss: 0.3454 - acc: 0.8946 - val_loss: 4.3681 - val_acc: 0.4079\n",
            "Epoch 17/20\n",
            "200/200 [==============================] - 118s 590ms/step - loss: 0.2569 - acc: 0.9238 - val_loss: 4.4370 - val_acc: 0.4157\n",
            "Epoch 18/20\n",
            "200/200 [==============================] - 117s 585ms/step - loss: 0.2107 - acc: 0.9346 - val_loss: 5.1317 - val_acc: 0.4236\n",
            "Epoch 19/20\n",
            "200/200 [==============================] - 118s 590ms/step - loss: 0.1851 - acc: 0.9464 - val_loss: 5.1190 - val_acc: 0.4223\n",
            "Epoch 20/20\n",
            "200/200 [==============================] - 117s 584ms/step - loss: 0.1850 - acc: 0.9476 - val_loss: 5.6971 - val_acc: 0.4191\n",
            "Maximal validation accuracy: 44.55%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-cNplHty5xc"
      },
      "source": [
        "plt.plot(baseline_history.history[\"acc\"], label=\"train_accuracy\")\n",
        "plt.plot(baseline_history.history[\"val_acc\"], label=\"val_accuracy\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Train and Validation Losses Over Epochs\", fontsize=14)\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwWVBcCWZIAM",
        "outputId": "7c49b7fb-197f-4c39-8386-9bf6228ae758"
      },
      "source": [
        "baseline_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"baseline_model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_16 (Conv2D)           (None, 47, 47, 128)       3584      \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 23, 23, 128)       147584    \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 11, 11, 128)       147584    \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 5, 5, 128)         147584    \n",
            "_________________________________________________________________\n",
            "flatten_6 (Flatten)          (None, 3200)              0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 10)                32010     \n",
            "=================================================================\n",
            "Total params: 478,346\n",
            "Trainable params: 478,346\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BPfU19s0kNP"
      },
      "source": [
        "def get_encoder():\n",
        "    return keras.Sequential(\n",
        "        [\n",
        "            keras.Input(shape=(image_size, image_size, image_channels)),\n",
        "            layers.Conv2D(width, kernel_size=3, strides=2, activation=\"relu\"),\n",
        "            layers.Conv2D(width, kernel_size=3, strides=2, activation=\"relu\"),\n",
        "            layers.Conv2D(width, kernel_size=3, strides=2, activation=\"relu\"),\n",
        "            layers.Conv2D(width, kernel_size=3, strides=2, activation=\"relu\"),\n",
        "            layers.Flatten(),\n",
        "            layers.Dense(width, activation=\"relu\"),\n",
        "        ],\n",
        "        name=\"encoder\",\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmQ15GHq0AhD"
      },
      "source": [
        "#contrastive learning model\n",
        "class ContrastiveModel(keras.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.temperature = temperature\n",
        "        self.contrastive_augmenter = get_augmenter(**contrastive_augmentation)\n",
        "        self.classification_augmenter = get_augmenter(**classification_augmentation)\n",
        "        self.encoder = get_encoder()\n",
        "        # Non-linear MLP as projection head\n",
        "        self.projection_head = keras.Sequential(\n",
        "            [\n",
        "                keras.Input(shape=(width,)),\n",
        "                layers.Dense(width, activation=\"relu\"),\n",
        "                layers.Dense(width),\n",
        "            ],\n",
        "            name=\"projection_head\",\n",
        "        )\n",
        "        # Single dense layer for linear probing\n",
        "        self.linear_probe = keras.Sequential(\n",
        "            [layers.Input(shape=(width,)), layers.Dense(10)], name=\"linear_probe\"\n",
        "        )\n",
        "\n",
        "        self.encoder.summary()\n",
        "        self.projection_head.summary()\n",
        "        self.linear_probe.summary()\n",
        "\n",
        "    def compile(self, contrastive_optimizer, probe_optimizer, **kwargs):\n",
        "        super().compile(**kwargs)\n",
        "\n",
        "        self.contrastive_optimizer = contrastive_optimizer\n",
        "        self.probe_optimizer = probe_optimizer\n",
        "\n",
        "        # self.contrastive_loss will be defined as a method\n",
        "        self.probe_loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "        self.contrastive_loss_tracker = keras.metrics.Mean(name=\"c_loss\")\n",
        "        self.contrastive_accuracy = keras.metrics.SparseCategoricalAccuracy(\n",
        "            name=\"c_acc\"\n",
        "        )\n",
        "        self.probe_loss_tracker = keras.metrics.Mean(name=\"p_loss\")\n",
        "        self.probe_accuracy = keras.metrics.SparseCategoricalAccuracy(name=\"p_acc\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [\n",
        "            self.contrastive_loss_tracker,\n",
        "            self.contrastive_accuracy,\n",
        "            self.probe_loss_tracker,\n",
        "            self.probe_accuracy,\n",
        "        ]\n",
        "\n",
        "    def contrastive_loss(self, projections_1, projections_2):\n",
        "        # InfoNCE loss (information noise-contrastive estimation)\n",
        "        # NT-Xent loss (normalized temperature-scaled cross entropy)\n",
        "\n",
        "        # Cosine similarity: the dot product of the l2-normalized feature vectors\n",
        "        projections_1 = tf.math.l2_normalize(projections_1, axis=1)\n",
        "        projections_2 = tf.math.l2_normalize(projections_2, axis=1)\n",
        "        similarities = (\n",
        "            tf.matmul(projections_1, projections_2, transpose_b=True) / self.temperature\n",
        "        )\n",
        "\n",
        "        # The similarity between the representations of two augmented views of the\n",
        "        # same image should be higher than their similarity with other views\n",
        "        batch_size = tf.shape(projections_1)[0]\n",
        "        contrastive_labels = tf.range(batch_size)\n",
        "        self.contrastive_accuracy.update_state(contrastive_labels, similarities)\n",
        "        self.contrastive_accuracy.update_state(\n",
        "            contrastive_labels, tf.transpose(similarities)\n",
        "        )\n",
        "\n",
        "        # The temperature-scaled similarities are used as logits for cross-entropy\n",
        "        # a symmetrized version of the loss is used here\n",
        "        loss_1_2 = keras.losses.sparse_categorical_crossentropy(\n",
        "            contrastive_labels, similarities, from_logits=True\n",
        "        )\n",
        "        loss_2_1 = keras.losses.sparse_categorical_crossentropy(\n",
        "            contrastive_labels, tf.transpose(similarities), from_logits=True\n",
        "        )\n",
        "        return (loss_1_2 + loss_2_1) / 2\n",
        "\n",
        "    def train_step(self, data):\n",
        "        (unlabeled_images, _), (labeled_images, labels) = data\n",
        "\n",
        "        # Both labeled and unlabeled images are used, without labels\n",
        "        images = tf.concat((unlabeled_images, labeled_images), axis=0)\n",
        "        # Each image is augmented twice, differently\n",
        "        augmented_images_1 = self.contrastive_augmenter(images)\n",
        "        augmented_images_2 = self.contrastive_augmenter(images)\n",
        "        with tf.GradientTape() as tape:\n",
        "            features_1 = self.encoder(augmented_images_1)\n",
        "            features_2 = self.encoder(augmented_images_2)\n",
        "            # The representations are passed through a projection mlp\n",
        "            projections_1 = self.projection_head(features_1)\n",
        "            projections_2 = self.projection_head(features_2)\n",
        "            contrastive_loss = self.contrastive_loss(projections_1, projections_2)\n",
        "        gradients = tape.gradient(\n",
        "            contrastive_loss,\n",
        "            self.encoder.trainable_weights + self.projection_head.trainable_weights,\n",
        "        )\n",
        "        self.contrastive_optimizer.apply_gradients(\n",
        "            zip(\n",
        "                gradients,\n",
        "                self.encoder.trainable_weights + self.projection_head.trainable_weights,\n",
        "            )\n",
        "        )\n",
        "        self.contrastive_loss_tracker.update_state(contrastive_loss)\n",
        "\n",
        "        # Labels are only used in evalutation for an on-the-fly logistic regression\n",
        "        preprocessed_images = self.classification_augmenter(labeled_images)\n",
        "        with tf.GradientTape() as tape:\n",
        "            features = self.encoder(preprocessed_images)\n",
        "            class_logits = self.linear_probe(features)\n",
        "            probe_loss = self.probe_loss(labels, class_logits)\n",
        "        gradients = tape.gradient(probe_loss, self.linear_probe.trainable_weights)\n",
        "        self.probe_optimizer.apply_gradients(\n",
        "            zip(gradients, self.linear_probe.trainable_weights)\n",
        "        )\n",
        "        self.probe_loss_tracker.update_state(probe_loss)\n",
        "        self.probe_accuracy.update_state(labels, class_logits)\n",
        "\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "    def test_step(self, data):\n",
        "        labeled_images, labels = data\n",
        "\n",
        "        # For testing the components are used with a training=False flag\n",
        "        preprocessed_images = self.classification_augmenter(\n",
        "            labeled_images, training=False\n",
        "        )\n",
        "        features = self.encoder(preprocessed_images, training=False)\n",
        "        class_logits = self.linear_probe(features, training=False)\n",
        "        probe_loss = self.probe_loss(labels, class_logits)\n",
        "        self.probe_loss_tracker.update_state(probe_loss)\n",
        "        self.probe_accuracy.update_state(labels, class_logits)\n",
        "\n",
        "        # Only the probe metrics are logged at test time\n",
        "        return {m.name: m.result() for m in self.metrics[2:]}\n",
        "\n",
        "\n",
        "# Contrastive pretraining\n",
        "pretraining_model = ContrastiveModel()\n",
        "pretraining_model.compile(\n",
        "    contrastive_optimizer=keras.optimizers.Adam(),\n",
        "    probe_optimizer=keras.optimizers.Adam(),\n",
        ")\n",
        "\n",
        "pretraining_history = pretraining_model.fit(\n",
        "    train_dataset, epochs=num_epochs, validation_data=test_dataset\n",
        ")\n",
        "print(\n",
        "    \"Maximal validation accuracy: {:.2f}%\".format(\n",
        "        max(pretraining_history.history[\"val_p_acc\"]) * 100\n",
        "    )\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUepGlFECoLP"
      },
      "source": [
        "Self-Supervised\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsySivPy1XQ0"
      },
      "source": [
        "plt.plot(pretraining_history.history[\"c_acc\"], label=\"train_accuracy\")\n",
        "plt.plot(pretraining_history.history[\"p_acc\"], label=\"val_accuracy\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Train and Validation Losses Over Epochs\", fontsize=14)\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQCE1Kgdomrf"
      },
      "source": [
        "# Define the contrastive model with model-subclassing\n",
        "class ContrastiveModel(keras.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.temperature = temperature\n",
        "        self.contrastive_augmenter = get_augmenter(**contrastive_augmentation)\n",
        "        self.classification_augmenter = get_augmenter(**classification_augmentation)\n",
        "        self.encoder = get_encoder()\n",
        "        # Non-linear MLP as projection head\n",
        "        self.projection_head = keras.Sequential(\n",
        "            [\n",
        "                keras.Input(shape=(width,)),\n",
        "                layers.Dense(width, activation=\"relu\"),\n",
        "                layers.Dense(width),\n",
        "            ],\n",
        "            name=\"projection_head\",\n",
        "        )\n",
        "        # Single dense layer for linear probing\n",
        "        self.linear_probe = keras.Sequential(\n",
        "            [layers.Input(shape=(width,)), layers.Dense(10)], name=\"linear_probe\"\n",
        "        )\n",
        "\n",
        "        self.encoder.summary()\n",
        "        self.projection_head.summary()\n",
        "        self.linear_probe.summary()\n",
        "\n",
        "    def compile(self, contrastive_optimizer, probe_optimizer, **kwargs):\n",
        "        super().compile(**kwargs)\n",
        "\n",
        "        self.contrastive_optimizer = contrastive_optimizer\n",
        "        self.probe_optimizer = probe_optimizer\n",
        "\n",
        "        # self.contrastive_loss will be defined as a method\n",
        "        self.probe_loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "        self.contrastive_loss_tracker = keras.metrics.Mean(name=\"c_loss\")\n",
        "        self.contrastive_accuracy = keras.metrics.SparseCategoricalAccuracy(\n",
        "            name=\"c_acc\"\n",
        "        )\n",
        "        self.probe_loss_tracker = keras.metrics.Mean(name=\"p_loss\")\n",
        "        self.probe_accuracy = keras.metrics.SparseCategoricalAccuracy(name=\"p_acc\")\n",
        "\n",
        "   \n",
        "    def metrics(self):\n",
        "        return [\n",
        "            self.contrastive_loss_tracker,\n",
        "            self.contrastive_accuracy,\n",
        "            self.probe_loss_tracker,\n",
        "            self.probe_accuracy,\n",
        "        ]\n",
        "\n",
        "    def contrastive_loss(self, projections_1, projections_2):\n",
        "        # InfoNCE loss (information noise-contrastive estimation)\n",
        "        # NT-Xent loss (normalized temperature-scaled cross entropy)\n",
        "\n",
        "        # Cosine similarity: the dot product of the l2-normalized feature vectors\n",
        "        projections_1 = tf.math.l2_normalize(projections_1, axis=1)\n",
        "        projections_2 = tf.math.l2_normalize(projections_2, axis=1)\n",
        "        similarities = (\n",
        "            tf.matmul(projections_1, projections_2, transpose_b=True) / self.temperature\n",
        "        )\n",
        "\n",
        "        # The similarity between the representations of two augmented views of the\n",
        "        # same image should be higher than their similarity with other views\n",
        "        batch_size = tf.shape(projections_1)[0]\n",
        "        contrastive_labels = tf.range(batch_size)\n",
        "        self.contrastive_accuracy.update_state(contrastive_labels, similarities)\n",
        "        self.contrastive_accuracy.update_state(\n",
        "            contrastive_labels, tf.transpose(similarities)\n",
        "        )\n",
        "\n",
        "        # The temperature-scaled similarities are used as logits for cross-entropy\n",
        "        # a symmetrized version of the loss is used here\n",
        "        loss_1_2 = keras.losses.sparse_categorical_crossentropy(\n",
        "            contrastive_labels, similarities, from_logits=True\n",
        "        )\n",
        "        loss_2_1 = keras.losses.sparse_categorical_crossentropy(\n",
        "            contrastive_labels, tf.transpose(similarities), from_logits=True\n",
        "        )\n",
        "        return (loss_1_2 + loss_2_1) / 2\n",
        "\n",
        "    def train_step(self, data):\n",
        "        (unlabeled_images, _), (labeled_images, labels) = data\n",
        "\n",
        "        # Both labeled and unlabeled images are used, without labels\n",
        "        images = tf.concat((unlabeled_images, labeled_images), axis=0)\n",
        "        # Each image is augmented twice, differently\n",
        "        augmented_images_1 = self.contrastive_augmenter(images)\n",
        "        augmented_images_2 = self.contrastive_augmenter(images)\n",
        "        with tf.GradientTape() as tape:\n",
        "            features_1 = self.encoder(augmented_images_1)\n",
        "            features_2 = self.encoder(augmented_images_2)\n",
        "            # The representations are passed through a projection mlp\n",
        "            projections_1 = self.projection_head(features_1)\n",
        "            projections_2 = self.projection_head(features_2)\n",
        "            contrastive_loss = self.contrastive_loss(projections_1, projections_2)\n",
        "        gradients = tape.gradient(\n",
        "            contrastive_loss,\n",
        "            self.encoder.trainable_weights + self.projection_head.trainable_weights,\n",
        "        )\n",
        "        self.contrastive_optimizer.apply_gradients(\n",
        "            zip(\n",
        "                gradients,\n",
        "                self.encoder.trainable_weights + self.projection_head.trainable_weights,\n",
        "            )\n",
        "        )\n",
        "        self.contrastive_loss_tracker.update_state(contrastive_loss)\n",
        "\n",
        "        # Labels are only used in evalutation for an on-the-fly logistic regression\n",
        "        preprocessed_images = self.classification_augmenter(labeled_images)\n",
        "        with tf.GradientTape() as tape:\n",
        "            features = self.encoder(preprocessed_images)\n",
        "            class_logits = self.linear_probe(features)\n",
        "            probe_loss = self.probe_loss(labels, class_logits)\n",
        "        gradients = tape.gradient(probe_loss, self.linear_probe.trainable_weights)\n",
        "        self.probe_optimizer.apply_gradients(\n",
        "            zip(gradients, self.linear_probe.trainable_weights)\n",
        "        )\n",
        "        self.probe_loss_tracker.update_state(probe_loss)\n",
        "        self.probe_accuracy.update_state(labels, class_logits)\n",
        "\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "    def test_step(self, data):\n",
        "        labeled_images, labels = data\n",
        "\n",
        "        # For testing the components are used with a training=False flag\n",
        "        preprocessed_images = self.classification_augmenter(\n",
        "            labeled_images, training=False\n",
        "        )\n",
        "        features = self.encoder(preprocessed_images, training=False)\n",
        "        class_logits = self.linear_probe(features, training=False)\n",
        "        probe_loss = self.probe_loss(labels, class_logits)\n",
        "        self.probe_loss_tracker.update_state(probe_loss)\n",
        "        self.probe_accuracy.update_state(labels, class_logits)\n",
        "\n",
        "        # Only the probe metrics are logged at test time\n",
        "        return {m.name: m.result() for m in self.metrics[2:]}\n",
        "\n",
        "\n",
        "# Contrastive pretraining\n",
        "pretraining_model = ContrastiveModel()\n",
        "pretraining_model.compile(\n",
        "    contrastive_optimizer=keras.optimizers.Adam(),\n",
        "    probe_optimizer=keras.optimizers.Adam(),\n",
        ")\n",
        "\n",
        "pretraining_history = pretraining_model.fit(\n",
        "    train_dataset, epochs=num_epochs, validation_data=test_dataset\n",
        ")\n",
        "print(\n",
        "    \"Maximal validation accuracy: {:.2f}%\".format(\n",
        "        max(pretraining_history.history[\"val_p_acc\"]) * 100\n",
        "    )\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfFAKwRmv2YO"
      },
      "source": [
        "fine - tuning\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYLt8L2uEEo9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f076f04-c271-4a88-8e74-84f32d3a0c80"
      },
      "source": [
        "# Supervised finetuning of the pretrained encoder\n",
        "finetuning_model = keras.Sequential(\n",
        "    [\n",
        "        layers.Input(shape=(image_size, image_size, image_channels)),\n",
        "        get_augmenter(**classification_augmentation),\n",
        "        pretraining_model.encoder,\n",
        "        layers.Dense(10),\n",
        "    ],\n",
        "    name=\"finetuning_model\",\n",
        ")\n",
        "finetuning_model.compile(\n",
        "    optimizer=keras.optimizers.Adam(),\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")],\n",
        ")\n",
        "\n",
        "finetuning_history = finetuning_model.fit(\n",
        "    labeled_train_dataset, epochs=num_epochs, validation_data=test_dataset\n",
        ")\n",
        "print(\n",
        "    \"Maximal validation accuracy: {:.2f}%\".format(\n",
        "        max(finetuning_history.history[\"val_acc\"]) * 100\n",
        "    )\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}